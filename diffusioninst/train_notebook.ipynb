{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import itertools\n",
    "import weakref\n",
    "from typing import Any, Dict, List, Set\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "from fvcore.nn.precise_bn import get_bn_modules\n",
    "\n",
    "\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import transforms as T\n",
    "import detectron2.utils.comm as comm\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.evaluation import COCOEvaluator, LVISEvaluator, verify_results\n",
    "from detectron2.solver.build import maybe_add_gradient_clipping\n",
    "from detectron2.modeling import build_model\n",
    "\n",
    "# from detectron2.data import transforms as T\n",
    "\n",
    "\n",
    "from diffusioninst import (\n",
    "    DiffusionInstDatasetMapper,\n",
    "    add_diffusioninst_config,\n",
    "    DiffusionInstWithTTA,\n",
    ")\n",
    "from diffusioninst.util.model_ema import (\n",
    "    add_model_ema_configs,\n",
    "    may_build_model_ema,\n",
    "    may_get_ema_checkpointer,\n",
    "    EMAHook,\n",
    "    apply_model_ema_and_restore,\n",
    "    EMADetectionCheckpointer,\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "from datadings.reader import MsgpackReader\n",
    "from torchvision.transforms import Compose, Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "iamge shape (256, 256, 3)\n",
      "{'image_id': 428223, 'image_width': 601, 'image_height': 792, 'image_file_path': '/ds-sds//documents/publaynet/publaynet/test/PMC3382231_00001.jpg', 'image': array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ...,\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]]], dtype=uint8), 'objects': [{'category_id': 0, 'image_id': '428223', 'id': 4196430, 'area': 3687, 'bbox': [50.73, 72.78, 240.2, 21.95], 'segmentation': [[50.73, 72.78, 290.94, 72.78, 290.94, 83.36, 151.59, 83.36, 151.59, 94.73, 50.73, 94.73, 50.73, 84.15, 50.73, 84.15, 50.73, 72.78]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 0, 'image_id': '428223', 'id': 4196431, 'area': 43162, 'bbox': [50.73, 97.51, 240.23, 181.09], 'segmentation': [[65.67, 97.51, 290.93, 97.51, 290.93, 108.88, 290.95, 108.88, 290.95, 119.46, 290.92, 119.46, 290.92, 131.61, 290.93, 131.61, 290.93, 142.98, 290.94, 142.98, 290.94, 153.56, 290.91, 153.56, 290.91, 165.72, 290.96, 165.72, 290.96, 176.3, 290.93, 176.3, 290.93, 187.66, 290.9, 187.66, 290.9, 199.82, 290.94, 199.82, 290.94, 211.19, 290.97, 211.19, 290.97, 221.77, 290.93, 221.77, 290.93, 233.92, 290.93, 233.92, 290.93, 245.29, 290.95, 245.29, 290.95, 255.87, 290.9, 255.87, 290.9, 267.23, 276.37, 267.23, 276.37, 278.6, 50.73, 278.6, 50.73, 268.02, 50.73, 256.65, 50.73, 244.5, 50.73, 244.5, 50.73, 233.13, 50.73, 233.13, 50.73, 222.55, 50.73, 210.4, 50.73, 210.4, 50.73, 199.82, 50.73, 188.45, 50.73, 177.08, 50.73, 177.08, 50.73, 165.72, 50.73, 154.35, 50.73, 142.98, 50.73, 131.61, 50.73, 120.25, 50.73, 108.88, 65.67, 108.88, 65.67, 97.51]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 0, 'image_id': '428223', 'id': 4196432, 'area': 63360, 'bbox': [50.73, 281.39, 240.24, 272.04], 'segmentation': [[65.67, 281.39, 290.93, 281.39, 290.93, 291.97, 290.92, 291.97, 290.92, 304.12, 290.93, 304.12, 290.93, 315.49, 290.95, 315.49, 290.95, 326.07, 290.91, 326.07, 290.91, 338.22, 290.97, 338.22, 290.97, 348.8, 290.92, 348.8, 290.92, 360.96, 290.95, 360.96, 290.95, 371.54, 290.93, 371.54, 290.93, 383.69, 290.93, 383.69, 290.93, 394.27, 290.92, 394.27, 290.92, 405.64, 290.92, 405.64, 290.92, 417.79, 290.92, 417.79, 290.92, 428.37, 290.9, 428.37, 290.9, 440.53, 290.95, 440.53, 290.95, 451.11, 290.95, 451.11, 290.95, 462.48, 290.9, 462.48, 290.9, 474.64, 290.94, 474.64, 290.94, 486.01, 290.94, 486.01, 290.94, 496.59, 290.9, 496.59, 290.9, 508.74, 290.93, 508.74, 290.93, 520.11, 290.96, 520.11, 290.96, 530.69, 290.94, 530.69, 290.94, 542.06, 131.55, 542.06, 131.55, 553.42, 50.73, 553.42, 50.73, 542.84, 50.73, 531.48, 50.73, 520.11, 50.73, 508.74, 50.73, 497.37, 50.73, 497.37, 50.73, 486.01, 50.73, 474.64, 50.73, 463.26, 50.73, 451.9, 50.73, 440.53, 50.73, 429.16, 50.73, 417.79, 50.73, 406.43, 50.73, 395.06, 50.73, 382.91, 50.73, 382.91, 50.73, 372.33, 50.73, 360.96, 50.73, 349.59, 50.73, 338.22, 50.73, 326.86, 50.73, 326.86, 50.73, 315.49, 50.73, 304.12, 50.73, 292.75, 65.67, 292.75, 65.67, 281.39]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 0, 'image_id': '428223', 'id': 4196433, 'area': 35265, 'bbox': [50.73, 595.9, 240.24, 146.99], 'segmentation': [[50.73, 595.9, 290.94, 595.9, 290.94, 606.49, 290.93, 606.49, 290.93, 618.64, 290.95, 618.64, 290.95, 629.22, 290.94, 629.22, 290.94, 640.59, 290.94, 640.59, 290.94, 651.95, 290.93, 651.95, 290.93, 664.11, 290.94, 664.11, 290.94, 674.69, 290.86, 674.69, 290.86, 686.84, 290.92, 686.84, 290.92, 698.21, 290.94, 698.21, 290.94, 709.58, 290.96, 709.58, 290.96, 720.94, 290.97, 720.94, 290.97, 731.52, 287.37, 731.52, 287.37, 742.89, 50.73, 742.89, 50.73, 732.31, 50.73, 720.94, 50.73, 709.58, 50.73, 698.21, 50.73, 686.84, 50.73, 675.47, 50.73, 663.32, 50.73, 663.32, 50.73, 651.95, 50.73, 651.95, 50.73, 639.25, 50.73, 630.01, 50.73, 618.64, 50.73, 618.64, 50.73, 606.49, 50.73, 606.49, 50.73, 595.9]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 0, 'image_id': '428223', 'id': 4196434, 'area': 18543, 'bbox': [308.66, 72.78, 240.25, 78.79], 'segmentation': [[308.66, 72.78, 548.91, 72.78, 548.91, 83.37, 548.84, 83.37, 548.84, 95.51, 548.87, 95.51, 548.87, 106.09, 548.87, 106.09, 548.87, 118.25, 548.91, 118.25, 548.91, 128.83, 548.85, 128.83, 548.85, 140.2, 515.27, 140.2, 515.27, 151.57, 308.66, 151.57, 308.66, 140.85, 308.66, 129.62, 308.66, 118.25, 308.66, 106.88, 308.66, 94.73, 308.66, 94.73, 308.66, 84.01, 308.66, 72.78]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 0, 'image_id': '428223', 'id': 4196435, 'area': 24300, 'bbox': [308.66, 166.92, 240.22, 101.52], 'segmentation': [[308.66, 166.92, 548.87, 166.92, 548.87, 177.5, 548.87, 177.5, 548.87, 189.65, 548.88, 189.65, 548.88, 200.23, 548.88, 200.23, 548.88, 211.6, 548.85, 211.6, 548.85, 223.76, 548.88, 223.76, 548.88, 234.34, 548.82, 234.34, 548.82, 246.49, 548.89, 246.49, 548.89, 257.07, 541.46, 257.07, 541.46, 268.44, 308.66, 268.44, 308.66, 257.86, 308.66, 246.49, 308.66, 235.12, 308.66, 223.76, 308.66, 212.39, 308.66, 212.39, 308.66, 201.02, 308.66, 188.87, 308.66, 188.87, 308.66, 178.14, 308.66, 166.92]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 0, 'image_id': '428223', 'id': 4196436, 'area': 31061, 'bbox': [308.66, 295.15, 240.22, 135.63], 'segmentation': [[308.66, 295.15, 548.85, 295.15, 548.85, 305.73, 548.84, 305.73, 548.84, 317.89, 548.85, 317.89, 548.85, 329.26, 548.85, 329.26, 548.85, 339.84, 548.85, 339.84, 548.85, 351.99, 548.89, 351.99, 548.89, 362.57, 548.88, 362.57, 548.88, 373.94, 548.82, 373.94, 548.82, 386.09, 548.83, 386.09, 548.83, 397.46, 548.86, 397.46, 548.86, 408.04, 548.84, 408.04, 548.84, 419.42, 415.54, 419.42, 415.54, 430.78, 308.66, 430.78, 308.66, 420.2, 308.66, 408.84, 308.66, 397.46, 308.66, 386.09, 308.66, 374.73, 308.66, 363.36, 308.66, 351.99, 308.66, 339.84, 308.66, 339.84, 308.66, 329.26, 308.66, 317.89, 308.66, 306.52, 308.66, 295.15]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 0, 'image_id': '428223', 'id': 4196437, 'area': 55628, 'bbox': [308.66, 446.13, 240.24, 237.92], 'segmentation': [[308.66, 446.13, 548.87, 446.13, 548.87, 457.5, 548.9, 457.5, 548.9, 468.08, 548.86, 468.08, 548.86, 480.23, 548.89, 480.23, 548.89, 490.81, 548.85, 490.81, 548.85, 502.97, 548.86, 502.97, 548.86, 514.19, 548.88, 514.19, 548.88, 524.92, 548.86, 524.92, 548.86, 536.74, 548.87, 536.74, 548.87, 548.43, 548.87, 548.43, 548.87, 559.8, 548.88, 559.8, 548.88, 570.38, 548.83, 570.38, 548.83, 581.75, 548.82, 581.75, 548.82, 593.9, 548.86, 593.9, 548.86, 605.27, 548.89, 605.27, 548.89, 615.85, 548.88, 615.85, 548.88, 627.47, 548.89, 627.47, 548.89, 638.58, 548.86, 638.58, 548.86, 650.73, 548.87, 650.73, 548.87, 662.1, 548.88, 662.1, 548.88, 672.68, 415.15, 672.68, 415.15, 684.05, 308.66, 684.05, 308.66, 672.68, 308.66, 672.68, 308.66, 662.1, 308.66, 650.73, 308.66, 650.73, 308.66, 638.85, 308.66, 638.85, 308.66, 627.47, 308.66, 616.63, 308.66, 605.27, 308.66, 593.9, 308.66, 582.53, 308.66, 571.17, 308.66, 559.8, 308.66, 548.43, 308.66, 548.43, 308.66, 536.74, 308.66, 524.92, 308.66, 524.92, 308.66, 514.19, 308.66, 502.97, 308.66, 491.6, 308.66, 480.23, 308.66, 468.87, 308.66, 457.5, 308.66, 446.13]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 2, 'image_id': '428223', 'id': 4196438, 'area': 4960, 'bbox': [338.55, 693.52, 210.27, 49.37], 'segmentation': [[338.55, 693.52, 548.8, 693.52, 548.8, 704.1, 355.73, 704.1, 355.73, 720.94, 548.82, 720.94, 548.82, 731.52, 358.06, 731.52, 358.06, 742.89, 338.55, 742.89, 338.55, 732.31, 338.55, 720.94, 338.55, 704.89, 338.55, 693.52]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 1, 'image_id': '428223', 'id': 4196439, 'area': 1642, 'bbox': [50.73, 574.19, 127.55, 12.88], 'segmentation': [[50.73, 574.19, 178.28, 574.19, 178.28, 587.06, 50.73, 587.06, 50.73, 574.19]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}, {'category_id': 1, 'image_id': '428223', 'id': 4196440, 'area': 2541, 'bbox': [308.66, 283.79, 240.18, 10.58], 'segmentation': [[308.66, 283.79, 548.85, 283.79, 548.85, 294.37, 308.66, 294.37, 308.66, 283.79]], 'iscrowd': False, 'bbox_mode': <BoxMode.XYXY_ABS: 0>}], 'instances': Instances(num_instances=1, image_height=256, image_width=256, fields=[gt_boxes: Boxes(tensor([[ 50.7300,  97.5100, 240.2300, 181.0900]])), gt_classes: tensor([0]), gt_masks: BitMasks(num_instances=1)])}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "\n",
    "class TorchMsgpackDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_path=\"/home/raushan/dataset/\",\n",
    "        split=\"train\",\n",
    "        transforms=None,\n",
    "        image_size=256,\n",
    "    ):\n",
    "        self.root_path = Path(root_path)\n",
    "        self.crop_gen = None\n",
    "        self.tfm_gens = []\n",
    "        self.data_reader = MsgpackReader(self.root_path / f\"publaynet-{split}.msgpack\")\n",
    "        if transforms is not None:\n",
    "            self.transforms = transforms\n",
    "        else:\n",
    "            self.transforms = Compose([Resize(image_size)])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data_reader[index]\n",
    "        # sample[\"objects\"][\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "        sample[\"image\"] = PIL.Image.open(io.BytesIO(sample[\"image\"][\"bytes\"]))\n",
    "        sample[\"image\"] = self.transforms(sample[\"image\"])\n",
    "        ## convert pil image to numpy array\n",
    "        sample[\"image\"] = np.array(sample[\"image\"])\n",
    "        ## change shape\n",
    "        image = sample[\"image\"]\n",
    "        image_shape = sample[\"image\"].shape\n",
    "        print(len(sample[\"objects\"]))\n",
    "        print(\"iamge shape\", image_shape)\n",
    "        annos = sample[\"objects\"]\n",
    "        for obj in annos:\n",
    "            obj[\"bbox_mode\"] = BoxMode.XYXY_ABS\n",
    "\n",
    "        # if np.random.rand() > 0.5:\n",
    "        #     image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n",
    "        # else:\n",
    "        #     image, transforms = T.apply_transform_gens(\n",
    "        #         self.tfm_gens[:-1] + self.crop_gen + self.tfm_gens[-1:], image\n",
    "        #     )\n",
    "        ## change this properly.\n",
    "        # annos = [\n",
    "        #     utils.transform_instance_annotations(obj, self.transforms, image_shape)\n",
    "        #     for obj in sample.pop(\"objects\")\n",
    "        #     if obj.get(\"iscrowd\", 0) == 0\n",
    "        # ]\n",
    "        instances = utils.annotations_to_instances(\n",
    "            annos, (image_shape[0], image_shape[1]), mask_format=\"bitmask\"\n",
    "        )\n",
    "        sample[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_reader)\n",
    "\n",
    "\n",
    "dataset = TorchMsgpackDataset(\n",
    "    root_path=\"/home/raushan/dataset/\",\n",
    "    split=\"train\",\n",
    "    transforms=None,\n",
    "    image_size=256,\n",
    ")\n",
    "# print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def seed_all_rng(seed=None):\n",
    "    \"\"\"\n",
    "    Set the random seed for the RNG in torch, numpy and python.\n",
    "\n",
    "    Args:\n",
    "        seed (int): if None, will use a strong random seed.\n",
    "    \"\"\"\n",
    "    if seed is None:\n",
    "        seed = (\n",
    "            os.getpid()\n",
    "            + int(datetime.now().strftime(\"%S%f\"))\n",
    "            + int.from_bytes(os.urandom(2), \"big\")\n",
    "        )\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Using a generated random seed {}\".format(seed))\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "def trivial_batch_collator(batch):\n",
    "    \"\"\"\n",
    "    A batch collator that does nothing.\n",
    "    \"\"\"\n",
    "    return batch\n",
    "\n",
    "\n",
    "def worker_init_reset_seed(worker_id):\n",
    "    initial_seed = torch.initial_seed() % 2**31\n",
    "    seed_all_rng(initial_seed + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = None\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def build_train_loader(cls, cfg):\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    # read msgpack\n",
    "    dataset = TorchMsgpackDataset(\n",
    "        root_path=\"/home/raushan/dataset/\",\n",
    "        split=\"train\",\n",
    "        transforms=None,\n",
    "        image_size=256,\n",
    "    )\n",
    "\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=8,\n",
    "        drop_last=True,\n",
    "        num_workers=2,\n",
    "        collate_fn=trivial_batch_collator if collate_fn is None else collate_fn,\n",
    "        worker_init_fn=worker_init_reset_seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m      2\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m TorchMsgpackDataset(\n\u001b[1;32m      3\u001b[0m         root_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/raushan/dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         transforms\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m         image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      7\u001b[0m     )\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m, in \u001b[0;36mTorchMsgpackDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[1;32m     20\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_reader[index]\n\u001b[0;32m---> 21\u001b[0m     sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m     22\u001b[0m     sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms(sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     23\u001b[0m     image_shape \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyError\u001b[0m: 'images'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    dataset = TorchMsgpackDataset(\n",
    "        root_path=\"/home/raushan/dataset/\",\n",
    "        split=\"test\",\n",
    "        transforms=None,\n",
    "        image_size=256,\n",
    "    )\n",
    "    dataset[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
